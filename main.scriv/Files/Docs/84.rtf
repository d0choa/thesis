{\rtf1\ansi\ansicpg1252\cocoartf1187\cocoasubrtf370
{\fonttbl\f0\fnil\fcharset0 Cochin;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural

\f0\fs28 \cf0 The [aforementioned methodologies](#coevolutionmethods) produce large lists of protein pairs sorted by the score of the corresponding method. Additionally, taking into account the [reference sets of interactions](#goldstandards), these protein pairs can be labeled as positives or negatives. A particular list will represent a better predictive power if the positives tend to cluster at the top of the ranking and worse if the positives and the negatives are randomly ranked. The question about whether to evaluate these lists is not trivial. Here we describe different approaches used for evaluation.\
\
The \'93receiver operating characteristic\'94 analysis (ROC[#Fawcett:2006gr]) illustrates the performance of a binary classifier as its discrimination threshold is varied. The ROC analysis generates a plot of \'93true positives rate\'94 (TPR) against \'93false positives rate\'94 (FPR) when varying the score of the predictor. Curves above the diagonal represent methods with some discriminative power, being more discriminant when the curves get close to the top-left corner of the plot. Therefore, the Area Under the ROC Curve (AUC) is usually calculated as a quantification of the global performance of the prediction, ranging from 0.5 (random classifier) to 1.0 (perfect classifier).\
\
In order to compare the ROC curves when the predictions contain different numbers of pairs, FPR and TPR are calculated respective to the total number of positives and negatives in the gold standard dataset. The curves defined this way, also called partial-ROC curves, provide not only an idea of the ability of the method to separate positives and negatives, but also of its range of applicability: whereas longer curves represent results with more predicted pairs, shorter curves are based on fewer observations.\
\
Both ROC curves and partial-ROC curves are generated by cutting the sorted list of scores at different thresholds and plotting the resulting TPRs against FPRs. The difference relies entirely on the way TPR and FPR are calculated:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
{\field{\*\fldinst{HYPERLINK "smb://["}}{\fldrslt \cf0 \\\\[}} TPR = Sensitivity = \\frac\{TP\}\{P\} = \\frac\{TP\}\{TP + FN\}\\\\] \
{\field{\*\fldinst{HYPERLINK "smb://["}}{\fldrslt \\\\[}} FPR = 1 - Specificity = \\frac\{FP\}\{N\} = \\frac\{FP\}\{FP + TN\} {\field{\*\fldinst{HYPERLINK "smb://]"}}{\fldrslt \\\\]}}\
\
\\\\(TP\\\\), \\\\(FP\\\\), \\\\(TN\\\\) and \\\\(FN\\\\) are the true positives, false positives, true negatives and false negatives according with a given threshold. The positives and negatives, \\\\(P\\\\) and \\\\(N\\\\) respectively, vary depending on whether we calculate the regular ROC curve, in which we calculate them from the pairs under evaluation, or the partial-ROC curve, in which case the positives and negatives are calculated from the dataset used for validation. Note that both parameters can also be interpreted in the context of \'93sensitivity\'94 and \'93specificity\'94, as indicated by the equations.\
\
ROC analysis measures the global ability to distinguish the positives from the negatives. In some cases, we may be only interested in predicting a few number of highly-confident interactions. Thus, an alternative analysis more focused on the top scoring pairs and on the positives needs to be performed. Another possible way of evaluating a sorted list focused on the positives is using \'93Precision\'94 and \'93Recall\'94. Considering a cut in the list of predictions produced by a given threshold, \'93Precision\'94 (also known as \'93positive predictive value\'94 - PPV) and \'93Recall\'94are defined as:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
{\field{\*\fldinst{HYPERLINK "smb://[Precision"}}{\fldrslt \cf0 \\\\[Precision}} = TP/(TP+FP) {\field{\*\fldinst{HYPERLINK "smb://]"}}{\fldrslt \\\\]}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
{\field{\*\fldinst{HYPERLINK "smb://[Recall"}}{\fldrslt \cf0 \\\\[Recall}} = TP/P {\field{\*\fldinst{HYPERLINK "smb://]"}}{\fldrslt \\\\]}}\
\
Usually, \'93Precision\'94 and \'93Recall\'94 are combined into a single parameter called \'93F-measure\'94, defined as the harmonic average of both parameters:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
{\field{\*\fldinst{HYPERLINK "smb://["}}{\fldrslt \cf0 \\\\[}} F-measure=\\frac\{2\\cdot Precision \\cdot Recall\}\{Precision + Recall\}\\\\]\
\
Usually, the \'93F-measure\'94 is represented against the scores of the predictor so as to detect the threshold with the best tradeoff between \'93Precission\'94 and \'93Recall\'94. The F-measure, obtained at this threshold, can be compared between different predictions obtaining an estimator of which is the best predictor in optimal circumstances. }